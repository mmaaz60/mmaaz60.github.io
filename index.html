
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

a {
color: #1772d0;
text-decoration: none;
}

a:focus,
a:hover {
color: #f09228;
text-decoration: none;
}

body,
td,
th,
tr,
p,
a {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px
}

strong {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
}

heading {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 22px;
}

papertitle {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
font-weight: 700
}

name {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 32px;
}

.one {
width: 160px;
height: 160px;
position: relative;
}

.two {
width: 160px;
height: 160px;
position: absolute;
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

.fade {
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

span.highlight {
background-color: #ffffd0;
}
  </style>
  <link rel="icon" type="image/png" href="img/icon.png">
  <title>Muhammad Maaz</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Muhammad Maaz</name>    <!-- The name -->
        </p>
        <p>
          I am a first year Ph.D. Computer Vision student at <a href="https://mbzuai.ac.ae/">MBZUAI</a> working under the
          supervision of <a href="https://scholar.google.com/citations?user=M59O9lkAAAAJ&hl=en">Dr. Salman
        </a> and <a href="https://scholar.google.com/citations?user=zvaeYnUAAAAJ&hl=en">Dr. Fahad</a>.
        </p>
        <p>
          My research is focused on developing multi-modal understanding from vision and text to improve common-sense reasoning of machines and its applications in open-vocabulary and open-world object detection. I am also exploring efficient neural networks for edge-computing devices (i.e. Jetson Nano).
        </p>
        <p>
          I received my B.Sc. degreen in Electrical Enginerring from <a href="https://www.uet.edu.pk/">UET Lahore</a> with honors in 2018. After my graduation I joined <a href="https://www.confiz.com/">Confiz Limited</a> as Computer Vision engineer where I worked on design and deployment of deep-learning driven computer vision solution for retail industry in Pakistan.
          In 2020, I joined to <a href="https://mbzuai.ac.ae/">MBZUAI</a> for pursuing my M.Sc. degree in Computer Vision. In 2022, I started my Ph.D. from MBZUAI.      </p>
        <p align=center>
          <a href="mailto:muhammad.maaz@mbzuai.ac.ae">Email</a> &nbsp/&nbsp
          <a href="pdf/MuhammadMaaz.pdf">CV</a> &nbsp/&nbsp
	        <a href="https://scholar.google.com/citations?user=vTy9Te8AAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
          <a href="https://www.github.com/mmaaz60">GitHub</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/mmaaz60">LinkedIn</a>
        </p>
        </td>
        <td width="33%">
	  <a href="img/profile.png">
        <img src="img/profile_circle.jpg" width="250px"></a>  <!-- Profile picture -->
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research and Publications</heading>
          <p>
            * denotes equal contribution co-authorship

          </p>
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='img/mavl.png' width="160">
        </div>
        <script type="text/javascript">
          function bhnerf_start() {
            document.getElementById('bhnerf_image').style.opacity = "1";
          }

          function bhnerf_stop() {
            document.getElementById('bhnerf_image').style.opacity = "0";
          }
          bhnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2111.11430/">
          <papertitle>Class-agnostic Object Detection with Multi-modal Transformer</papertitle>
        </a>
        <br>
        <strong>Muhammad Maaz*</strong>,
        <a href="https://scholar.google.com/citations?user=yhDdEuEAAAAJ&hl=en&authuser=1&oi=sra/">Hanoona Rasheed*</a>,
        <a href="https://salman-h-khan.github.io/">Salman Khan</a>,
        <a href="https://scholar.google.es/citations?user=zvaeYnUAAAAJ&hl=en/">Fahad Shahbaz Khan</a>,
        <a href="https://scholar.google.com/citations?hl=en&authuser=1&user=_KlvMVoAAAAJ/">Rao Muhammad Anwer</a>
        <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=en/">Ming-Hsuan Yang</a>
        <br>
  <em>ECCV</em>, 2022
        <br>
        <a href="https://github.com/mmaaz60/mvits_for_class_agnostic_od">project page</a>
  /
        <a href="https://arxiv.org/abs/2111.11430">arXiv</a>
  /
        <a href="https://youtu.be/pkooyDZAxdA">video</a>
        <p></p>
        <p>In this work, we explore the potential of the recent Multi-modal Vision Transformers (MViTs) for class-agnostic object detection. Our extensive experiments across various domains and novel objects show the state-of-the-art performance of MViTs to localize generic objects in images. We also develop an efficient and flexible MViT architecture using multi-scale feature processing and deformable self-attention that can adaptively generate proposals given a specific language query.</p>
      </td>
    </tr>

    <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='img/ovd.png' width="160">
        </div>
        <script type="text/javascript">
          function bhnerf_start() {
            document.getElementById('bhnerf_image').style.opacity = "1";
          }

          function bhnerf_stop() {
            document.getElementById('bhnerf_image').style.opacity = "0";
          }
          bhnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2207.03482/">
          <papertitle>Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection</papertitle>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=yhDdEuEAAAAJ&hl=en&authuser=1&oi=sra/">Hanoona Rasheed*</a>,
        <strong>Muhammad Maaz*</strong>,
        <a href="https://scholar.google.com/citations?user=M6fFL4gAAAAJ&hl=en&authuser=1">Muhammad Uzair Khattak</a>
        <a href="https://salman-h-khan.github.io/">Salman Khan</a>,
        <a href="https://scholar.google.es/citations?user=zvaeYnUAAAAJ&hl=en/">Fahad Shahbaz Khan</a>,
        <br>
  <em>NeurIPS</em>, 2022
        <br>
        <a href="https://hanoonar.github.io/object-centric-ovd/">project page</a>
  /
        <a href="https://arxiv.org/abs/2207.03482">arXiv</a>
  /
        <a href="https://youtu.be/QLlxulFV0KE">video</a>
        <p></p>
        <p>In this work, we propose to solve the Open-vocabulary detection (OVD) problem using pretrained CLIP model,
          adapting it for object-centric local regions using region-based distillation and image-level weak supervision.
          Specifically, we propose to utilize high-quality class-agnostic and class-specific object proposals via the pretrained
          mulit-modal vision transformers (MViT). The class-agnostic proposals are used to distill region-specific
          information from CLIP and class-specific proposals allows us to visually ground large vocabularies. We also
          introduce a region-conditioned weight transfer method to get complementary benefits from both region-based
          distillation and image-level supervision.</p>
      </td>
    </tr>

    <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='img/edgenext.png' width="160">
        </div>
        <script type="text/javascript">
          function bhnerf_start() {
            document.getElementById('bhnerf_image').style.opacity = "1";
          }

          function bhnerf_stop() {
            document.getElementById('bhnerf_image').style.opacity = "0";
          }
          bhnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2206.10589/">
          <papertitle>EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications</papertitle>
        </a>
        <br>
        <strong>Muhammad Maaz*</strong>,
        <a href="https://scholar.google.com/citations?hl=en&user=eEz4Wu4AAAAJ/">Abdelrahman Shaker,*</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=bZ3YBRcAAAAJ/">Hisham Cholakkal</a>,
        <a href="https://salman-h-khan.github.io/">Salman Khan</a>,
        <a href="https://www.waqaszamir.com/">Syed Waqas Zamir</a>,
        <a href="https://scholar.google.com/citations?hl=en&authuser=1&user=_KlvMVoAAAAJ/">Rao Muhammad Anwer</a>,
        <a href="https://scholar.google.es/citations?user=zvaeYnUAAAAJ&hl=en/">Fahad Shahbaz Khan</a>,
        <br>
  <em>CADL (ECCVW)</em>, 2022
        <br>
        <a href="https://mmaaz60.github.io/EdgeNeXt/">project page</a>
  /
        <a href="https://arxiv.org/abs/2206.10589/">arXiv</a>
  /
        <a href="https://www.youtube.com/watch?v=Oh-ooHlx58o">video</a>
        <p></p>
        <p>In this work, we designed resource-efficient general purpose backbone network for vision tasks. We combine
          the strengths of both CNN and Transformer models and propose a new efficient hybrid architecture EdgeNeXt.
          Specifically in EdgeNeXt, we introduce split depth-wise transpose attention (SDTA) encoder that splits input
          tensors into multiple channel groups and utilizes depth-wise convolution along with self-attention across
          channel dimensions to implicitly increase the receptive field and encode multi-scale features. Our extensive
          experiments on classification, detection and segmentation tasks, reveal the merits of the proposed approach,
          outperforming state-of-the-art methods with comparatively lower compute requirements.</p>
      </td>
    </tr>

    <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='img/MaPLe.png' width="160">
        </div>
        <script type="text/javascript">
          function bhnerf_start() {
            document.getElementById('bhnerf_image').style.opacity = "1";
          }

          function bhnerf_stop() {
            document.getElementById('bhnerf_image').style.opacity = "0";
          }
          bhnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2210.03117">
          <papertitle>MaPLe: Multi-modal Prompt Learning</papertitle>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=M6fFL4gAAAAJ&hl=en&authuser=1">Muhammad Uzair Khattak</a>,
        <a href="https://scholar.google.com/citations?user=yhDdEuEAAAAJ&hl=en&authuser=1&oi=sra/">Hanoona Rasheed</a>,
        <strong>Muhammad Maaz</strong>,
        <a href="https://salman-h-khan.github.io/">Salman Khan</a>,
        <a href="https://scholar.google.es/citations?user=zvaeYnUAAAAJ&hl=en/">Fahad Shahbaz Khan</a>,
        <br>
  <em>Under Review</em>, 2022
        <br>
        <a href="https://muzairkhattak.github.io/multimodal-prompt-learning/">project page</a>
  /
        <a href="https://arxiv.org/abs/2210.03117">arXiv</a>
        <p></p>
        <p>In this work, we propose to learn prompts in both vision and language branches of pretrained CLIP for
          adapting it to different downstream tasks. Previous works only use prompting in either language or vision
          branch. We note that using prompting to adapt representations in a single branch of CLIP (language or vision)
          is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a
          downstream task. To this end, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language
          branches to improve alignment between the vision and language representations. Our design promotes strong
          coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent
          uni-modal solutions.</p>
      </td>
    </tr>

      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          You've probably seen this website template before, thanks to <a href="https://jonbarron.info">Jon Barron</a>. <br>
          Last updated May 2020.
          <!-- http://www.cs.princeton.edu/~namana/ http://people.csail.mit.edu/janner/ http://www.sjoerdvansteenkiste.com/-->
	    </font>
        </p>
        </td>
      </tr>
      </table>

    </td>
    </tr>
  </table>
  </body>
</html>
